{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df = pd.read_csv(\"credit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df.head(10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df['default'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only for discussion about how encoding will be done\n",
    "np.sort(credit_df['checking_balance'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df.info()  # many columns are of type object i.e. strings. These need to be converted to ordinal type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for feature in credit_df.columns: # Loop through all columns in the dataframe\n",
    "    if credit_df[feature].dtype == 'object': # Only apply for columns with categorical strings\n",
    "        credit_df[feature] = le.fit_transform(credit_df[feature].astype(str))\n",
    "        print(feature)\n",
    "        print(dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "        print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for feature in credit_df.columns: # Loop through all columns in the dataframe\n",
    "#    if credit_df[feature].dtype == 'object': # Only apply for columns with categorical strings\n",
    "#        credit_df[feature] = pd.Categorical(credit_df[feature]).codes # Replace strings with an integer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple methods exist\n",
    "# Label Encoder  \n",
    "\n",
    "# OR\n",
    "\n",
    "# manually encoding as shown below ***** this has certain advantages \n",
    "# credit_df['checking_balance'] = credit_df['checking_balance'].replace({'< 0 DM':0, '1 - 200 DM':1, '> 200 DM':2, 'unknown':3 })\n",
    "\n",
    "\n",
    "# Get dummies can also be used it will have different impact on the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting feature variable to X\n",
    "X = credit_df.drop('default',axis=1)\n",
    "\n",
    "# Putting response variable to y\n",
    "y = credit_df['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoking the decision tree classifier function. \n",
    "#Restricting the depth of the tree to 10 (no particular reason for selecting this)\n",
    "\n",
    "                                  \n",
    "dt_model = DecisionTreeClassifier( max_depth = 10, random_state = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "y_pred = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Printing confusion matrix and accuracy\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training and testing score\n",
    "# Training Score\n",
    "print('Training score')\n",
    "print(dt_model.score(X_train,y_train))\n",
    "print('Test score')\n",
    "print(dt_model.score(X_test , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Features importance in the tree building ( The importance of a feature is computed as the \n",
    "#(normalized) total reduction of the criterion brought by that feature)\n",
    "\n",
    "print (pd.DataFrame(dt_model.feature_importances_, columns = [\"Imp\"], index = X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = ['No', 'Yes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following is Not useful to visualize large trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****\n",
    "# ***** This code can be used to print small trees. Large trees are not properly visible *****\n",
    "# ***** Therefore the code is commented\n",
    "# *****\n",
    "\n",
    "#from sklearn.tree import plot_tree\n",
    "#plt.figure(figsize=(70,35))\n",
    "#a=plot_tree(dt_model,feature_names=list(X_train), class_names = list(class_label),fontsize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize tree  using Graphviz website \n",
    "\n",
    "from IPython.display import Image  \n",
    "#import pydotplus as pydot\n",
    "from sklearn import tree\n",
    "from os import system\n",
    "\n",
    "Credit_Tree_File = open('credit_tree_1.dot','w')\n",
    "dot_data = tree.export_graphviz(dt_model, out_file=Credit_Tree_File, feature_names = list(X_train), class_names = list(class_label))\n",
    "#dot_data = tree.export_graphviz(dt_model, out_file=Credit_Tree_File, feature_names = list(X_train))\n",
    "Credit_Tree_File.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://webgraphviz.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With Hyper Parameters Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#making the instance\n",
    "model= DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "# hyper parameters to build the model on\n",
    "params = {'max_depth': range(1, 20)}\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model1 = GridSearchCV(model, param_grid=params, cv=n_folds, return_train_score=True)\n",
    "#Learning\n",
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture scores from Grid search\n",
    "scores = model1.cv_results_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy against max_depth\n",
    "plt.figure()\n",
    "plt.plot(scores[\"param_max_depth\"], \n",
    "         scores[\"mean_train_score\"], \n",
    "         label=\"training accuracy\")\n",
    "plt.plot(scores[\"param_max_depth\"], \n",
    "         scores[\"mean_test_score\"], \n",
    "         label=\"test accuracy\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameter\n",
    "print(\"Best Hyper Parameters:\",model1.best_params_)\n",
    "print(\"Best Score:\",model1.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune multiple papameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune MULTIPLE parameters with Hyper Parameters Tuning\n",
    "\n",
    "#create an instance\n",
    "model= DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# hyper parameters to build the model on\n",
    "params = {'criterion':('gini','entropy'), \n",
    "          'max_depth': range(2, 10),\n",
    "         'min_samples_split': range(2, 10)\n",
    "         }\n",
    "\n",
    "#Making models with hyper parameters sets\n",
    "model1 = GridSearchCV(model, cv=5, param_grid=params)\n",
    "#Learning\n",
    "model1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Hyper Parameters:\",model1.best_params_)\n",
    "print(\"Best Score:\",model1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensemble Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier( max_depth = 5, random_state=0)\n",
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print (pd.DataFrame(dt_model.feature_importances_, columns = [\"Imp\"], index = X_train.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.score(X_test , y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning - Bagging Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bgcl = BaggingClassifier(base_estimator=dt_model, n_estimators=20, random_state=10)\n",
    "\n",
    "bgcl = bgcl.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bgcl.predict(X_test)\n",
    "bgcl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble  learning - RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfcl = RandomForestClassifier(n_estimators = 20, random_state=0)\n",
    "rfcl = rfcl.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = rfcl.predict(X_test)\n",
    "rfcl.score(X_test , y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hyperparameter tuning of RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'n_estimators': range(60,160,10) ,\n",
    "    'max_depth': [4,8,12] ,\n",
    "    'max_features':[4,5]\n",
    "    \n",
    "    }\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier(random_state=10)\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv=5, \n",
    "                           n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the optimal accuracy score and hyperparameters\n",
    "print('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
